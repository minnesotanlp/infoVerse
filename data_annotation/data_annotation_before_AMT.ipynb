{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import easydict\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from models import load_backbone, Classifier\n",
    "from data import get_base_dataset\n",
    "from utils import Logger, set_seed, set_model_path, save_model, load_augment, add_mislabel_dataset\n",
    "from training.common import cut_input, get_embed, data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jaehyung/workspace/infoverse/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scores_src import get_features, merge_multiple_models\n",
    "from src.scores_src import avg_conf_variab, avg_forgetting, avg_aum\n",
    "from src.scores_src import get_density_score, get_mlm_scores, masking_dataset, get_mlm_scores_jh, get_sentence_embedding, compute_nearest_neighbour_distances_cls\n",
    "from src.scores_src import confidence, entropy, badge_grads_norm\n",
    "from src.scores_src.ensembles import mc_dropout_models, el2n_score, ens_max_ent, ens_bald, ens_varR\n",
    "from src.scores_src.dpp import gaussian_kernel, dpp_greedy, dpp_sampling\n",
    "from src.scores_src.info import aggregate, get_infoverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\"batch_size\": 2, \n",
    "                          \"backbone\": 'roberta_large',\n",
    "                          \"dataset\": 'imp',\n",
    "                          \"ood_dataset\": 'trec',\n",
    "                          \"train_type\": 'base',\n",
    "                          \"aug_type\": 'none',\n",
    "                          \"seed\": 1234,\n",
    "                          \"name\": '0417_base_large',\n",
    "                          \"pre_ckpt\": './logs/imp_R1.0_0417_base_large_S1234/imp_roberta-large_0417_base_large_epoch4.model',\n",
    "                          \"score_type\": 'confidence',\n",
    "                          \"topK\": True,\n",
    "                          \"data_ratio\": 1.0,\n",
    "                          \"n_classes\": 2,\n",
    "                        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding syntactic noise label to train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In Transformers v4.0.0, the default path to cache downloaded models changed from '~/.cache/torch/transformers' to '~/.cache/huggingface/transformers'. Since you don't seem to have overridden and '~/.cache/torch/transformers' is a directory that exists, we're moving it to '~/.cache/huggingface/transformers' to avoid redownloading models you have already in the cache. You should only see this message once.\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-large and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "backbone, tokenizer = load_backbone(args.backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing base dataset... (name: imp)\n"
     ]
    }
   ],
   "source": [
    "dataset, train_loader, val_loader, test_loader = get_base_dataset(args.dataset, tokenizer, args.batch_size, args.data_ratio, args.seed, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset.train_dataset, shuffle=False, drop_last=False, batch_size=args.batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_v = dataset.val_dataset[:][1][:, 0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_t = dataset.train_dataset[:][1][:, 0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(args.backbone, backbone, dataset.n_classes, args.train_type).cuda()\n",
    "if args.pre_ckpt is not None:\n",
    "    model.load_state_dict(torch.load(args.pre_ckpt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imp_reduce_5K.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import advertools as adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = []\n",
    "\n",
    "for sample in data:\n",
    "    emoji_summary = adv.extract_emoji(sample)\n",
    "    n_emoji = len(emoji_summary['emoji_flat'])\n",
    "    \n",
    "    if n_emoji == 0:\n",
    "        reduced_data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(sentences, tokenizer):\n",
    "    n_samples = len(sentences)\n",
    "    \n",
    "    tokens_aft = []\n",
    "    for i in range(n_samples):\n",
    "        token_aft = tokenizer.encode(sentences[i], add_special_tokens=True, max_length=128,\n",
    "                                             pad_to_max_length=True, return_tensors='pt')\n",
    "        tokens_aft.append(token_aft)\n",
    "    return torch.cat(tokens_aft, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoModelForCausalLM, RobertaTokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(gpt_model, gpt_tokenizer, text_data):\n",
    "    ppls = []\n",
    "    stride, max_length = 512, 512 \n",
    "    \n",
    "    for text in text_data:\n",
    "        sample_token = gpt_tokenizer.encode(text, return_tensors='pt').cuda()\n",
    "        \n",
    "        nlls = []\n",
    "        for i in range(0, sample_token.size(1), stride):\n",
    "            begin_loc = max(i + stride - max_length, 0)\n",
    "            end_loc = min(i + stride, sample_token.size(1))\n",
    "            trg_len = end_loc - i  # may be different from stride on last loop\n",
    "            input_ids = sample_token[:, begin_loc:end_loc].cuda()\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = gpt_model(input_ids, labels=target_ids)\n",
    "                neg_log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "        ppls.append(float(ppl.cpu()))\n",
    "        \n",
    "    return ppls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_ppls = perplexity(gpt_model, gpt_tokenizer, reduced_data)\n",
    "gpt_ppls = np.array(gpt_ppls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_idx = []\n",
    "for i in range(len(gpt_ppls)):\n",
    "    if gpt_ppls[i] >= 1000:\n",
    "        continue\n",
    "    else:\n",
    "        trimmed_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def repetitions(s):\n",
    "   r = re.compile(r\"(.+?)\\1+\")\n",
    "   for match in r.finditer(s):\n",
    "       yield (match.group(1), len(match.group(0))/len(match.group(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repetition(sent):\n",
    "    lists = list(repetitions(sent))\n",
    "    n_max = 0\n",
    "    \n",
    "    for item in lists:\n",
    "        n_repeat = item[1]\n",
    "        \n",
    "        if n_repeat > n_max:\n",
    "            n_max = n_repeat\n",
    "    \n",
    "    return n_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_data = []\n",
    "for i in range(len(reduced_data)):\n",
    "    n_repeat = get_repetition(reduced_data[i])\n",
    "    if n_repeat < 5 and len(reduced_data[i]) < 512:\n",
    "        trimmed_data.append(reduced_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/jaehyung/anaconda3/envs/whatsup/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokens_data = tokenization(trimmed_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pseudo(tokens, model):\n",
    "    model.eval()\n",
    "    \n",
    "    batch_size = 16\n",
    "    n_sample = len(tokens) \n",
    "    n_batch = int(n_sample / batch_size) + int(n_sample % batch_size != 0)\n",
    "    \n",
    "    all_confs, all_labels, all_penuls = [], [], []\n",
    "    for i in range(n_batch):\n",
    "        tokens_i = tokens[i*batch_size:(i+1)*batch_size].cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, penuls = model(tokens_i, get_penul=True)\n",
    "            confs, pseudo_labels = torch.softmax(logits, dim=1).max(dim=1)\n",
    "            \n",
    "        all_confs.append(confs.cpu())\n",
    "        all_labels.append(pseudo_labels.cpu())\n",
    "        all_penuls.append(penuls.cpu())\n",
    "    return torch.cat(all_confs, dim=0), torch.cat(all_labels, dim=0), torch.cat(all_penuls, dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs, pseudo, penuls = get_pseudo(tokens_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_indices = torch.arange(len(tokens_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = TensorDataset(tokens_data, pseudo.unsqueeze(1), new_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loader = DataLoader(new_dataset, shuffle=False, drop_last=False, batch_size=args.batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InfoVerse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-large and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaehyung/workspace/WhatsUp/scores_src/others.py:130: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  mask_idx = mask.nonzero()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "infoverse = get_infoverse(args, label_dataset=dataset.train_dataset, pool_dataset=new_dataset,\n",
    "                     n_epochs=7, seeds_list=[1234, 2345, 3456], n_class=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_t2 = (infoverse - infoverse.mean(axis=0)) / (1e-8 + infoverse.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 % of processing has been done\n"
     ]
    }
   ],
   "source": [
    "selected_idx = dpp_sampling(1100, measures_t2, pseudo.numpy(), scores='inv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('imp_info_5k_selected_idx.npy', selected_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = np.load('imp_info_5k_selected_idx.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sent = []\n",
    "\n",
    "for idx in selected_idx:\n",
    "    sent = trimmed_data[idx]\n",
    "    \n",
    "    equal = 0\n",
    "    for item in final_sent:\n",
    "        if sent == item:\n",
    "            equal += 1\n",
    "    \n",
    "    if equal == 0:\n",
    "        final_sent.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jaehyung/imp_unlabel_infoverse.txt', 'w') as f:\n",
    "    temp = 1\n",
    "    for item in final_sent[:1000]:\n",
    "        f.write(str(temp) + '\\t' + item)\n",
    "        temp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpp_sampling(n_query, measurements, labels, scores='density', reduce=False):\n",
    "    n_sample = len(measurements)\n",
    "    eps = 5e-4\n",
    "\n",
    "    # Dimension reduction for removing redundant features\n",
    "    if reduce:\n",
    "        info_measures, _ = PPCA(measurements)\n",
    "    else:\n",
    "        info_measures = np.array(measurements)\n",
    "\n",
    "    # Define similarity kernel phi(x_1, x_2)\n",
    "    similarity = gaussian_kernel(info_measures / np.linalg.norm(info_measures, axis=-1).reshape(-1, 1))\n",
    "\n",
    "    # Define score function q(x)\n",
    "    if scores == 'density':\n",
    "        scores_bef = -1 * compute_nearest_neighbour_distances_cls(info_measures, labels, info_measures, labels, nearest_k=5)\n",
    "        scores = (-1 / (1e-8 + scores_bef))\n",
    "    elif scores == 'inv':\n",
    "        scores = compute_nearest_neighbour_distances_cls(info_measures, labels, info_measures, labels, nearest_k=5)\n",
    "    else:\n",
    "        scores = np.ones(n_sample)\n",
    "    scores = (scores - scores.min()) / scores.max()\n",
    "\n",
    "    dpp_kernel = scores.reshape((n_sample, 1)) * similarity * scores.reshape((1, n_sample))\n",
    "    selected_idx = dpp_greedy(dpp_kernel + eps * np.eye(n_sample), n_query)\n",
    "\n",
    "    return selected_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = list(torch.randperm(len(trimmed_data))[:1100].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sent = []\n",
    "\n",
    "for idx in random_idx:\n",
    "    sent = trimmed_data[idx]\n",
    "    \n",
    "    equal = 0\n",
    "    for item in final_sent:\n",
    "        if sent == item:\n",
    "            equal += 1\n",
    "    \n",
    "    if equal == 0:\n",
    "        final_sent.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jaehyung/imp_unlabel_random.txt', 'w') as f:\n",
    "    temp = 1\n",
    "    for item in final_sent[:1000]:\n",
    "        f.write(str(temp) + '\\t' + item)\n",
    "        temp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain_idx = torch.Tensor(infoverse[:, 6]).sort(descending=True)[1][:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sent = []\n",
    "\n",
    "for idx in uncertain_idx:\n",
    "    sent = trimmed_data[idx]\n",
    "    \n",
    "    equal = 0\n",
    "    for item in final_sent:\n",
    "        if sent == item:\n",
    "            equal += 1\n",
    "    \n",
    "    if equal == 0:\n",
    "        final_sent.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jaehyung/imp_unlabel_uncertain.txt', 'w') as f:\n",
    "    temp = 1\n",
    "    for item in final_sent[:1000]:\n",
    "        f.write(str(temp) + '\\t' + item)\n",
    "        temp += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whatsup",
   "language": "python",
   "name": "whatsup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
