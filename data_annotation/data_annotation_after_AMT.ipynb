{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import easydict\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jaehyung/workspace/infoverse/')\n",
    "\n",
    "from src.models import load_backbone, Classifier\n",
    "from src.training.common import AverageMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding syntactic noise label to train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 21:17:38.779993: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jaehyung/torch/install/lib:/home/jaehyung/torch/install/lib:/usr/local/cuda/lib64:/home/jaehyung/torch/install/lib:/home/jaehyung/torch/install/lib:/usr/local/cuda/lib64:\n",
      "2023-06-12 21:17:38.780047: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "In Transformers v4.0.0, the default path to cache downloaded models changed from '~/.cache/torch/transformers' to '~/.cache/huggingface/transformers'. Since you don't seem to have overridden and '~/.cache/torch/transformers' is a directory that exists, we're moving it to '~/.cache/huggingface/transformers' to avoid redownloading models you have already in the cache. You should only see this message once.\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-large and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "_, tokenizer = load_backbone('roberta_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_inputs(csv_file, tokenizer, exc=False, n_samples = 1001):\n",
    "    res_numpy_all = np.zeros((n_samples, 128))\n",
    "    if exc:\n",
    "        n_hit = len(csv_file) - 1\n",
    "    else:\n",
    "        n_hit = len(csv_file)\n",
    "    \n",
    "    id_idx = ['Input.id0', 'Input.id1', 'Input.id2', 'Input.id3', 'Input.id4', \n",
    "              'Input.id5', 'Input.id6', 'Input.id7', 'Input.id8', 'Input.id9']\n",
    "    input_idx = ['Input.text0', 'Input.text1', 'Input.text2',  'Input.text3', 'Input.text4',\n",
    "                 'Input.text5', 'Input.text6', 'Input.text7', 'Input.text8', 'Input.text9']\n",
    "    \n",
    "    for i in range(len(id_idx)):\n",
    "        idx = csv_file[id_idx[i]]\n",
    "        inputs = csv_file[input_idx[i]]\n",
    "        \n",
    "        for j in range(n_hit):\n",
    "            input_j = tokenizer.encode(inputs[j], add_special_tokens=True, max_length=128, pad_to_max_length=True, return_tensors='pt')\n",
    "            res_numpy_all[int(idx[j])] = input_j\n",
    "    \n",
    "    return res_numpy_all[1:].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_numpy(csv_file, data, n_samples = 1001):\n",
    "    res_numpy = np.zeros(n_samples)\n",
    "    res_numpy_all = np.zeros((n_samples, 7))\n",
    "    \n",
    "    n_counts = np.zeros(n_samples)\n",
    "    n_hit = len(csv_file) - 1\n",
    "    \n",
    "    id_idx = ['Input.id0', 'Input.id1', 'Input.id2', 'Input.id3', 'Input.id4', \n",
    "              'Input.id5', 'Input.id6', 'Input.id7', 'Input.id8', 'Input.id9']\n",
    "    if data == 'sst5':\n",
    "        score_idx = ['Answer.howMuch0', 'Answer.howMuch1', 'Answer.howMuch2',\n",
    "           'Answer.howMuch3', 'Answer.howMuch4', 'Answer.howMuch5',\n",
    "           'Answer.howMuch6', 'Answer.howMuch7', 'Answer.howMuch8',\n",
    "           'Answer.howMuch9']\n",
    "    else:\n",
    "        score_idx = ['Answer.insult0.insult0', 'Answer.insult1.insult1',\n",
    "           'Answer.insult2.insult2', 'Answer.insult3.insult3',\n",
    "           'Answer.insult4.insult4', 'Answer.insult5.insult5',\n",
    "           'Answer.insult6.insult6', 'Answer.insult7.insult7',\n",
    "           'Answer.insult8.insult8', 'Answer.insult9.insult9']\n",
    "    \n",
    "    for i in range(len(id_idx)):\n",
    "        idx = csv_file[id_idx[i]]\n",
    "        score = csv_file[score_idx[i]]\n",
    "        \n",
    "        for j in range(n_hit):\n",
    "            res_numpy[int(idx[j])] += int(score[j])\n",
    "            res_numpy_all[int(idx[j])][int(n_counts[int(idx[j])])] = int(score[j])\n",
    "            n_counts[int(idx[j])] += 1\n",
    "    \n",
    "    return res_numpy[1:], n_counts[1:], res_numpy_all[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disagree(all_votes, num_votes):\n",
    "    n_samples = len(all_votes)\n",
    "    \n",
    "    disagree = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        disagree[i] = np.std(all_votes[i, :int(num_votes[i])])\n",
    "    \n",
    "    return disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disagree_sst5(all_votes, num_votes):\n",
    "    n_samples = len(all_votes)\n",
    "    \n",
    "    all_votes_hard = np.array(all_votes)\n",
    "    for i in range(5):\n",
    "        i_idx = (5 * i <= all_votes) * (all_votes < 5 * (i+1))\n",
    "        all_votes_hard[i_idx] = i\n",
    "    \n",
    "    all_votes_hard /= 5\n",
    "    \n",
    "    disagree = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        disagree[i] = np.std(all_votes_hard[i, :int(num_votes[i])])\n",
    "    \n",
    "    return disagree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SST5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_csv = pd.read_csv('./anno_files/sst5_random_anno.csv')\n",
    "infoverse_csv = pd.read_csv('./anno_files/sst5_info_anno.csv')\n",
    "uncertain_csv = pd.read_csv('./anno_files/sst5_uncertain_anno.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sst5_labels_from_csv(csv_file):\n",
    "    sum_votes, num_votes, all_votes = convert_csv_to_numpy(csv_file, 'sst5')\n",
    "    \n",
    "    avg_votes = sum_votes / num_votes\n",
    "    \n",
    "    final_labels = np.zeros(len(avg_votes))\n",
    "    n_labels = np.zeros(5)\n",
    "    for i in range(5):\n",
    "        i_idx = (5 * i <= avg_votes) * (avg_votes < 5 * (i+1))\n",
    "        final_labels[i_idx] = i\n",
    "        n_labels[i] = (i_idx).sum()\n",
    "    \n",
    "    # Get annotation disagreements\n",
    "    disagreements = disagree_sst5(all_votes, num_votes)\n",
    "    \n",
    "    print(n_labels)\n",
    "    return final_labels, disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 22. 142. 434. 273. 128.]\n"
     ]
    }
   ],
   "source": [
    "random_labels, random_disagree = get_sst5_labels_from_csv(random_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 33. 184. 376. 310.  96.]\n"
     ]
    }
   ],
   "source": [
    "uncertain_labels, uncertain_disagree = get_sst5_labels_from_csv(uncertain_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 29. 164. 399. 268. 136.]\n"
     ]
    }
   ],
   "source": [
    "infoverse_labels, infoverse_disagree = get_sst5_labels_from_csv(infoverse_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_inputs = convert_csv_inputs(random_csv, tokenizer, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain_inputs = convert_csv_inputs(uncertain_csv, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "infoverse_inputs = convert_csv_inputs(infoverse_csv, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_csv = pd.read_csv('./anno_files/imp_random_anno.csv')\n",
    "infoverse_csv = pd.read_csv('./anno_files/imp_info_anno.csv')\n",
    "uncertain_csv = pd.read_csv('./anno_files/imp_uncertain_anno.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imp_labels_from_csv(csv_file):\n",
    "    sum_votes, num_votes, all_votes = convert_csv_to_numpy(csv_file, 601)\n",
    "    \n",
    "    avg_votes = sum_votes / num_votes\n",
    "    \n",
    "    final_labels = np.zeros(len(avg_votes))\n",
    "    n_labels = np.zeros(2)\n",
    "    \n",
    "    i_idx = (avg_votes < 0.5)\n",
    "    final_labels[i_idx] = 0\n",
    "    n_labels[0] = (i_idx).sum()\n",
    "    \n",
    "    i_idx2 = (avg_votes >= 0.5)\n",
    "    final_labels[i_idx2] = 1\n",
    "    n_labels[1] = (i_idx2).sum()\n",
    "    \n",
    "    # Get annotation disagreements\n",
    "    disagreements = disagree(all_votes, num_votes)\n",
    "    \n",
    "    print(n_labels)\n",
    "    return final_labels, disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[562.  38.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaehyung/anaconda3/envs/pytorch1.6/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n",
      "/home/jaehyung/anaconda3/envs/pytorch1.6/lib/python3.7/site-packages/numpy/core/_methods.py:262: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims, where=where)\n",
      "/home/jaehyung/anaconda3/envs/pytorch1.6/lib/python3.7/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  subok=False)\n",
      "/home/jaehyung/anaconda3/envs/pytorch1.6/lib/python3.7/site-packages/numpy/core/_methods.py:253: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "random_labels, random_disagree = get_imp_labels_from_csv(random_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[363. 237.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaehyung/anaconda3/envs/pytorch1.6/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "uncertain_labels, uncertain_disagree = get_imp_labels_from_csv(uncertain_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[373. 227.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaehyung/anaconda3/envs/pytorch1.6/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "infoverse_labels, infoverse_disagree = get_imp_labels_from_csv(infoverse_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_inputs = convert_csv_inputs(random_csv, tokenizer, False, 601)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain_inputs = convert_csv_inputs(uncertain_csv, tokenizer, False, 601)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "infoverse_inputs = convert_csv_inputs(infoverse_csv, tokenizer, False, 601)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Annoted Samples as Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_dataset(inputs, labels):\n",
    "    inputs = torch.LongTensor(inputs)\n",
    "    labels = torch.LongTensor(labels).unsqueeze(1)  # (N, 1)\n",
    "    index = torch.arange(len(inputs))\n",
    "\n",
    "    dataset = TensorDataset(inputs, labels, index)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dataset = create_tensor_dataset(random_inputs, random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain_dataset = create_tensor_dataset(uncertain_inputs, uncertain_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "infoverse_dataset = create_tensor_dataset(infoverse_inputs, infoverse_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(random_dataset, './anno_files/imp_random.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(uncertain_dataset, './anno_files/imp_uncertain.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(infoverse_dataset, './anno_files/imp_infoverse.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.6",
   "language": "python",
   "name": "pytorch1.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
